# monitoring/check_model_quality.py
import pandas as pd
import os
from sklearn.metrics import roc_auc_score, f1_score
import json
from datetime import datetime, timedelta
import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à–∏–π –ø–æ—Ä–æ–≥
import joblib
best_threshold = joblib.load("models/lightgbm_best_threshold.pkl")

def check_model_quality():
    # –ò—â–µ–º —Ñ–∞–π–ª —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ –ª–µ–π–±–ª–∞–º–∏
    label_file = "monitoring/logs/predictions_with_labels.csv"
    if not os.path.exists(label_file):
        print("‚ÑπÔ∏è –ù–µ—Ç —Ñ–∞–π–ª–∞ —Å –ª–µ–π–±–ª–∞–º–∏ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞")
        return None

    df = pd.read_csv(label_file)
    required_cols = {"score", "true_label"}
    if not required_cols.issubset(df.columns):
        print(f"‚ö†Ô∏è –ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ {required_cols} –≤ {label_file}")
        return None

    df = df.dropna(subset=["score", "true_label"])
    if len(df) < 10:  #  –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –º–µ–Ω—å—à–µ –¥–ª—è –∏—Å–ø—ã—Ç–∞–Ω–∏–π –∏ –ø—Ä–æ–≤–µ—Ä–æ–∫ 
        print("‚ÑπÔ∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö —Å –ª–µ–π–±–ª–∞–º–∏")
        return None

    y_true = df["true_label"].astype(int)
    y_pred_proba = df["score"]
    #y_pred = (y_pred_proba >= 0.5).astype(int)  # –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∞—à –ø–æ—Ä–æ–≥
    y_pred = (y_pred_proba >= best_threshold).astype(int)

    auc = roc_auc_score(y_true, y_pred_proba)
    f1 = f1_score(y_true, y_pred)

    print(f"üéØ ROC-AUC: {auc:.4f}")
    print(f"üéØ F1-score: {f1:.4f}")

    log_entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "component": "model_quality",
        "roc_auc": float(auc),
        "f1_score": float(f1),
        "n_samples": len(df)
    }

    with open("monitoring/drift_logs/drift_log.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")

    return auc, f1

if __name__ == "__main__":
    check_model_quality()